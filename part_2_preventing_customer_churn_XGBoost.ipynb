{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Customer Churn, Part 2. Building the ML Model\n",
    "\n",
    "----\n",
    "## Table of contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Exploration](#Data-Exploration)\n",
    "4. [Model Training](#Model-Training)\n",
    "5. [Feature Importance](#Feature-Importance)\n",
    "6. [Model Hosting](#Model-Hosting)\n",
    "7. [Model Evaluation](#Model-Evaluation)\n",
    "8. [Updated model](#Updated-model)\n",
    "9. [Summary](#Summary)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "_This notebook has been adapted from [Amazon SageMaker Examples - Customer Churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn). That notebook had been adapted from the [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/) and [AWS notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)._\n",
    "\n",
    "*In this version of the notebook, in addition to building the predictive model we explore: what are the factors that affect churn? How do we create a targeted incentive that we (as the provider) think is most likely to reduce churn, with the minimum cost to us? In another enhancement, we use Amazon SageMaker [Hyperarameter Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) to find better tuning parameters for our XGBoost model.*\n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early on gives the business a chance to offer them incentives to stay.  This notebook describes using machine learning (ML) for the automated identification of unhappy customers, also known as customer churn prediction. ML models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML.\n",
    "\n",
    "We use an example of churn that is familiar to all of us – leaving a mobile phone operator.  Seems like I can always find fault with my provider du jour! And if my provider knows that I’m thinking of leaving, it can offer timely incentives – I can always use a phone upgrade or perhaps have a new feature activated – and I might just stick around. Incentives are often much more cost effective than losing and reacquiring a customer.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an __ml.m4.xlarge__ notebook instance._\n",
    "\n",
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "# To get the container for training\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "# To run predictions against the model \n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# Data manipulations:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To un-pickle the model after training\n",
    "import pickle as pkl\n",
    "\n",
    "# To work with trees from xgboost\n",
    "import json\n",
    "\n",
    "# To mix code and markdown\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we are using a container with xgboost version 0.90-1. In order to successfully un-pickle the model artifact we need to install the corresponding version of xgboost, because `pip` will install the newest version of the module by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "# as we are training the model using version 0.90-1 of the container \n",
    "# we need to install the version of xgboost that matches even if it is not the latest one.\n",
    "!{sys.executable} -m pip install xgboost==0.90\n",
    "!{sys.executable} -m pip install graphviz\n",
    "# graphviz for plotting an interactive graph of relevant features\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "# verify module version\n",
    "print(f\"Installed version of xgboost library is {xgboost.__version__}\")\n",
    "# for creating a feature tree\n",
    "from xgboost import plot_tree\n",
    "# for calculating feature importance\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this use case, we've created the S3 bucket and appropriate IAM roles for you during the launch of the AWS CloudFormation template. The bucket name was saved in a parameter file called \"cloudformation_values.py\" during creation of the notebook instance, along with the DB secret name and ML endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudformation_values as cfvalues\n",
    "bucket = cfvalues.S3BUCKET\n",
    "endpoint_name = cfvalues.ENDPOINT\n",
    "# AWS Secrets stores our database credentials. \n",
    "db_secret_name = cfvalues.DBSECRET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountid = boto3.client('sts').get_caller_identity()['Account']\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# provide a prefix to be attached to the output files in the bucket\n",
    "prefix = 'sagemaker/xgboost-churn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom functions for data and feature exploration\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ColorBrewer\n",
    "plot_color = \"#4daf4a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Exploration\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets. \n",
    "\n",
    "#### Loading the data from Amazon S3\n",
    "\n",
    "In the Part 1 notebook for this use case we demonstrated unloading the production data from Amazon Aurora to an S3 bucket. Here we will read the unloaded data from the S3 bucket into a Pandas dataframe.\n",
    "\n",
    "In this case, there's a small amount of data and it was unloaded as a single file with a header, so the column names exist as part of the data. We'll explore this data and split it into test, train and validation sets locally. \n",
    "\n",
    "With large amounts of data, Amazon Aurora will create multiple files. In that case we could choose to explore one part only to get a basic understanding of the data, and use the multiple part files to create a 'natural' split into test etc. sets for us. In that case we likely would not have the column headers as part of the file, and would need to add them manually, as shown by the two commented out lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'aurora/churn_data.part_00000'\n",
    "\n",
    "data_location = 's3://{}/{}'.format(bucket, prefix + '/' + data_key)\n",
    "\n",
    "# If we needed to add column names manually, we'd need to get the column names from another metadata \n",
    "# source (or from our friendly DBA).\n",
    "# churn_cols = ['state','acc_length','area_code','phone','int_plan','vmail_plan','vmail_msg',\n",
    "#  'day_mins','day_calls','day_charge','eve_mins','eve_calls','eve_charge','night_mins',\n",
    "#  'night_calls','night_charge','int_mins','int_calls','int_charge','cust_service_calls','churn']\n",
    "# Use this format if the names were passed manually.\n",
    "# churn = pd.read_csv(data_location, index_col = 0, header = None, names = churn_cols) \n",
    "churn = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 25)\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modern standards, it’s a relatively small dataset, with only 3,333 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `state`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `acc_length`: the number of days that this account has been active\n",
    "- `area_code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `phone`: the remaining seven-digit phone number\n",
    "- `int_plan`: whether the customer has an international calling plan: yes/no\n",
    "- `vmail_plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `vmail_msg`: presumably the average number of voice mail messages per month\n",
    "- `day_mins`: the total number of calling minutes used during the day\n",
    "- `day_calls`: the total number of calls placed during the day\n",
    "- `day_charge`: the billed cost of daytime calls\n",
    "- `eve_mins, eve_calls, even_charge`: the billed cost for calls placed during the evening\n",
    "- `night_mins`, `night_calls`, `night_charge`: the billed cost for calls placed during nighttime\n",
    "- `int_mins`, `int_calls`, `int_charge`: the billed cost for international calls\n",
    "- `curs_service_calls`: the number of calls placed to Customer Service\n",
    "- `churn`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `churn`, is known as the target attribute–the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "#### Exploring the Data\n",
    "\n",
    "Let's begin exploring the data. \n",
    "\n",
    "_This section is identical to the original notebook, [Amazon SageMaker Examples - Customer Churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn). While data exploration is an important topic, it's not the focus of this walk through. Therefore it's been removed in the interests of brevity, but the actions taken based on the analysis have been kept (i.e., columns kept/removed). Please refer to the original notebook for this section._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('phone', axis = 1)\n",
    "churn['area_code'] = churn['area_code'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's remove one feature from each of the highly correlated pairs: \n",
    " 1. `day_charge` from the pair with `day_mins`; \n",
    " 2. `night_charge` from the pair with `night_mins`; \n",
    " 3. `eve_charge` from the pair with the `eve_mins`;\n",
    " 4. `int_mins` from the pair with `int_charge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['day_charge', 'eve_charge', 'night_charge', 'int_mins'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's determine which algorithm to use.  As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn.  In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms.  Instead, let's attempt to model this problem using gradient boosted trees.  Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint.  XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format.  For this example, we'll stick with CSV.  It should (documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)):\n",
    "\n",
    "- Have the predictor variable in the first column\n",
    "- Not have a header row\n",
    "\n",
    "Since the model will not have the feature names later, when we explore the results, we will need to assign them from the original data (excluding the target variable)\n",
    "\n",
    "But first, let's convert our categorical features into numeric features as the algorithm manages only numeric features. Then, we place the outcome as the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['churn_True.'], \n",
    "                        model_data.drop(['churn_False.', 'churn_True.'], axis = 1)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's split the data into training, validation, and test sets.  This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen.\n",
    "\n",
    "_Note that different splits of the data may create slightly different results. In addition, on different runs against the same data, XGBoost may choose different combinations of features and trees that give similar model performance._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac = 1, random_state = 1729), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header = False, index = False)\n",
    "validation_data.to_csv('validation.csv', header = False, index = False)\n",
    "test_data.to_csv('test.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training with XGBoost using HPO\n",
    "\n",
    "Now we'll train our XGBoost model, using HPO (hyperparameter optimization) to explore a number of hyperparameters, looking for the best model.\n",
    "\n",
    "First we'll need to specify the locations of the XGBoost algorithm containers. XGBoost can be used in two ways: either as a built-in algorithm or as a framework, see the documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html). Since we are using it as a container it is important to install the corresponding version of xgboost library for the analyses below (see the section [Feature Importance](#Feature-Importance)). At the point of the writing this notebook there were two versions of the container available: 0.90-1 and 0.90-2. All training here was performed with the version 0.90-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're always cautious and curious, we'll check the sizes of the data splits, to make sure we have a meaningful number of records in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model data shape', model_data.shape)\n",
    "print('Train data:', train_data.shape)\n",
    "print('Validation data:', validation_data.shape)\n",
    "print('Test data:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version = '0.90-1')\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data = 's3://{}/{}/train'.format(bucket, prefix),\n",
    "                               content_type = 'csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data = 's3://{}/{}/validation/'.format(bucket, prefix),\n",
    "                                    content_type = 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('s3://{}/{}/train'.format(bucket, prefix))\n",
    "print('s3://{}/{}/validation/'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on the GitHub [page](https://github.com/awsdocs/amazon-sagemaker-developer-guide/blob/master/doc_source/xgboost-tuning.md) for Amazon SageMaker Developer Guide.\n",
    "\n",
    "Ideally, a data scientist would perform k-fold cross-validation procedure to identify the set of parameters that produce the best performing model, but for brevity, we are not doing it here.\n",
    "\n",
    "We set initial hyperparameters, then set ranges of the hyperparameters that we'd like to try. These parameters are described further [here](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst), and the hyperparameters are described [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                               role,\n",
    "                               train_instance_count=1,\n",
    "                               train_instance_type = 'ml.m4.xlarge',\n",
    "                               output_path = 's3://{}/{}/output'.format(bucket, prefix),\n",
    "                               sagemaker_session = sess)\n",
    "xgb.set_hyperparameters(max_depth = 5,\n",
    "                        eta = 0.2,\n",
    "                        gamma = 4,\n",
    "                        min_child_weight = 6,\n",
    "                        subsample = 0.8,    \n",
    "                        silent = 0,\n",
    "                        objective = 'binary:logistic',\n",
    "                        num_round = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10),\n",
    "                        'num_round': IntegerParameter(10,100)}\n",
    "\n",
    "# The built-in XGBoost algorithm emits two predefined metrics: validation:auc and train:auc\n",
    "objective_metric_name = 'validation:auc'       \n",
    "\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs = 30,\n",
    "                            max_parallel_jobs = 3,\n",
    "                            early_stopping_type = 'Auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "tuning_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will wait while the set of HPO jobs are run. They will take approx. 30 minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check current status of hyperparameter tuning job\n",
    "region = boto3.Session().region_name\n",
    "sm_client = boto3.Session().client('sagemaker')\n",
    "\n",
    "tuning_job_result = sm_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name)\n",
    "\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "print(f\"{job_count} training jobs have completed\")\n",
    "    \n",
    "is_minimize = (tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the hyperparameters used and metrics returns by the best training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "if tuning_job_result.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_result['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_training_job = tuning_job_result['BestTrainingJob']['TrainingJobName']\n",
    "\n",
    "response = sm_client.describe_training_job(\n",
    "    TrainingJobName=best_training_job\n",
    ")\n",
    "print('Hyperparameters:')\n",
    "pprint(response['HyperParameters'], indent = 4)\n",
    "print('Metrics:')\n",
    "pprint(response['FinalMetricDataList'], indent = 4)\n",
    "themodel = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "print()\n",
    "print('Model location:',themodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract indices of the training and validation AUCs:\n",
    "train_res = utilities.get_auc_from_metrics(response, \"train:auc\")\n",
    "val_res = utilities.get_auc_from_metrics(response, \"validation:auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "The AUC looks pretty good. In our sample run, we saw a training AUC of \n",
    "{round(response['FinalMetricDataList'][train_res]['Value'], 3)}, and a validation\n",
    "AUC of {round(response['FinalMetricDataList'][val_res]['Value'], 3)}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note, due to randomized elements of the XGBoost algorithm and of SageMaker hyperparameter optimization, your results may differ slightly._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At this point, you can use the [Analyze HPO Tuning Job](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) sample notebook to further study the effectiveness of the hyperparameter tuning job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Feature Importance\n",
    "\n",
    "Now that we have a model, we'd like to understand what factors are of highest impact in our model. By understanding these factors, we hope to be able to intervene in order to prevent customer churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyze the model we will copy the model saved on S3 bucket during training to the notebook. We will then load it using the `load` function from the `pickle` module. `xgb.model_data` provides the location (uri) of the *.tar.gz file in the S3 bucket. \n",
    "\n",
    "In order to sucessfully un-pickle the file ensure that you have the matching version of xgboost library installed for the version of the SageMaker XGBoost container used above. (See the first few code blocks of this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the model locally to the notebook\n",
    "!aws s3 cp $themodel .\n",
    "# unpack the file, this will produce file \"xgboost-model\"\n",
    "!tar -zxvf model.tar.gz -C ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_booster = pkl.load(open(\"xgboost-model\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_booster` is an object of class `xgboost.core.Booster`. It has many methods available, see [the documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html). However, because we don't have the feature names available during training, we will first assign them here to replace non-descriptive labels \"f*\". Make sure to drop the first column name from the data, as it's the outcome variable (churn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_booster.feature_names = list(model_data.columns)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the most important features found by this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we present bar plots with features ranked by their importance. There are 5 types of feature importance available in xgboost library: gain, weight, cover, total gain and total cover. See the [API reference](https://xgboost.readthedocs.io/en/latest/python/python_api.html?highlight=get_score#xgboost.Booster.get_score) for descriptions. For each bar plot we choose to show the top features. \n",
    "\n",
    "From the [XGBoost tutorial](https://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html) the following explanations are provided for the measures of feature importance:\n",
    "\n",
    "```\n",
    "Gain (a.k.a. Gini feature importance) is the improvement in accuracy brought by a feature to the branches it is on. The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements, after adding the split on this feature, there are two new branches, and each of these branches is more accurate (one branch saying if your observation is on this branch then it should be classified as 1, and the other branch saying the exact opposite).\n",
    "\n",
    "Cover measures the relative quantity of observations concerned by a feature.\n",
    "\n",
    "Frequency (a.k.a. weight) is a simpler way to measure the Gain. It just counts the number of times a feature is used in all generated trees. You should not use it (unless you know why you want to use it).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We defined a custom function plot_feature_importance that plots up to 15 features ranked by their\n",
    "# importance using either gain, cover or weight. The function relies on xgboost's plot_importance method.\n",
    "\n",
    "# gain: the average gain across all splits the feature is used in.\n",
    "# weight: the number of times a feature is used to split the data across all trees.\n",
    "# cover: the average coverage across all splits the feature is used in.\n",
    "# total_gain: the total gain across all splits the feature is used in.\n",
    "# total_cover: the total coverage across all splits the feature is used in.\n",
    "for metric in ['gain', 'cover', 'weight']:    \n",
    "    utilities.plot_feature_importance(model_booster, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our runs, three features frequently dominate: `day_mins`, `cust_service_calls`, and international plan usage. \n",
    "\n",
    "_Note, due to randomized elements of the algorithm, your results may differ._\n",
    "\n",
    "While we can see that these features are the most important, the measure does not tell us the direction: for example, are a higher number of day_mins predictive of churn, or a lower number? Is int_plan_no (a Boolean value) True, or False, predictive? For that, additional exploration and analysis is needed. Often, the business will have a good intuition that can be validated from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to better understand what the model is doing. To get some better insight, let's plot some of the model trees.\n",
    "\n",
    "We'll start with the first tree. In XGBoost, each additional tree is added to maximize the gain, so looking at the first tree should give us some insight into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_plot = 0  \n",
    "the_tree = plot_tree(model_booster, num_trees=tree_to_plot, rankdir='LR')\n",
    "fig_size = plt.gcf().get_size_inches()      #Get current size\n",
    "sizefactor = 8     #Set a zoom factor\n",
    "# Modify the current size by the factor\n",
    "plt.gcf().set_size_inches(sizefactor * fig_size) \n",
    "\n",
    "# The plots can be hard to read (known issue). So, separately save it to a PNG, which makes for easier viewing.\n",
    "fig = plt.gcf()\n",
    "fig.savefig('tree' + str(tree_to_plot)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! In our runs, we saw the potential to use these trees to suggest or understand different kinds of churners. We can easily plot additional trees, or pull a list of the key splits, such as the top few splits of the various trees. Showing this kind of information to Marketing may give them some ideas for different \"churn profiles\" amongst customer segments, leading to different targeted incentive programs. \n",
    "\n",
    "In our runs we frequently see this first tree splitting on cust_service_calls = 3.5. Let's do a little exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn['csc_4ormore'] = churn.apply(lambda x: 1 if x['cust_service_calls'] >= 4 else 0, axis=1)\n",
    "pd.crosstab(churn['csc_4ormore'], churn['churn'], normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little data exploration supports the idea that 4 or more calls shift the percentage of churners. We’ll likely want to intervene before they make that fourth phone call.\n",
    "\n",
    "We can also draw additional trees. Let's get a sense of the number and size of the other trees we have in this model. In past XGBoost projects, we've seen that in some cases - primarily where the data has little information, or where the model is not converging well - some or even many or most of the trees may contain only a single leaf, and so do not add any decision power to the model. Let's see whether that's affecting this model.   \n",
    "\n",
    "In our module `utilities` we define functions to calculate the depth of each tree and return the number of tree with each value of depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all trees in json format\n",
    "tree_dump = model_booster.get_dump(dump_format = 'json')\n",
    "\n",
    "# Get depth for each tree\n",
    "all_depths = utilities.get_depths_as_list(tree_dump)\n",
    "\n",
    "# Calculate the distribution of tree depths for plotting\n",
    "unique_tree_count = utilities.calculate_list_unique_elements(all_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are: \" + str(len(tree_dump)) + \" trees in this forest.\")\n",
    "plt.grid(b = True,  which = 'major', axis = 'y', color = \"lightgrey\")\n",
    "tree_depth_barplot = plt.bar(unique_tree_count.keys(), \n",
    "                      unique_tree_count.values(), \n",
    "                      color = \"#4daf4a\")\n",
    "yticks = plt.yticks(np.arange(0, max(unique_tree_count.values())+2, step = 5))\n",
    "ylabel = plt.ylabel('Trees')\n",
    "xlabel = plt.xlabel(\"Depth\")\n",
    "title = plt.title(\"Distribution of tree depths for all trees in the model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test run, several trees contain only a leaf, so it seems some further optimization is possible. But for now we'll move on.\n",
    "\n",
    "We're really interested in how our features have been used across the trees, to get a sense of which of our features influence the model. \n",
    "\n",
    "In our module `utilities` we define additional functions that allow us to find all trees where features have been used to split the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = utilities.count_trees_with_features(tree_dump, model_booster.feature_names)\n",
    "# convert it to pandas series object\n",
    "ser1 = pd.Series(feature_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = plt.figure(figsize = [12,6])\n",
    "plt.grid(b = True,  which = 'major', axis = 'both', color = \"lightgrey\")\n",
    "feature_splits_scatterplot = plt.scatter(ser1.sort_values().index, \n",
    "                      ser1.sort_values().values, \n",
    "                      color = \"#4daf4a\")\n",
    "plt.yticks(np.arange(0, max(ser1.sort_values().values) + 5, step = 5))\n",
    "plt.xticks(rotation = 90, fontsize = 9)\n",
    "plt.ylabel('Trees')\n",
    "plt.xlabel(\"Features\")\n",
    "plt.title(\"How many times is each feature used to create a split?\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the figure above we observe that the majority of features are not even used in the tree splits. These features are adding no value to the model. Here are the features that are used at least once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1[ser1 != 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "Here, we see opportunity! Whereas the training data had 70 features after converting the categorical\n",
    "variables to one-hot vectors, only {ser1[ser1 != 0].size} features are being used by the model. \n",
    "Perhaps we can simplify the model, which will simplify the calls from the production database as well. \n",
    "We'll try that after we've finished evaluating this model.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Hosting\n",
    "\n",
    "Now that we've trained the algorithm, let's create a model and deploy it to a hosted endpoint. \n",
    "\n",
    "Note that we're using a predefined endpoint_name here, for simplicity. The AWS CloudFormation template setup specified this endpoint_name in an IAM role, and set the Aurora DB cluster group parameter 'aws_default_sagemaker_role' to this IAM role. This combination of settings gives Aurora permission to call the Amazon SageMaker endpoint that we'll be creating here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = tuner.deploy(initial_instance_count = 1,\n",
    "                             instance_type = 'ml.m4.xlarge',\n",
    "                             endpoint_name = endpoint_name,\n",
    "                             update_endpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command may take several minutes to respond, while we wait for the endpoint for be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model Evaluation\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  Next, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, predictor_object, rows = 200):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor_object.predict(array).decode('utf-8')])\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.values[:, 1:], xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values.  In this case, we're simply predicting whether the customer churned (`1`) or not (`0`), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(index = test_data.iloc[:, 0], \n",
    "                               columns = np.round(predictions), \n",
    "                               rownames = ['actual'],\n",
    "                               colnames = ['predictions'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "_Note, due to randomized elements of the algorithm, your results may differ slightly._\n",
    "\n",
    "In this run, of the 48 churners, we've correctly predicted {confusion_matrix.iloc[1][1]} of them (true positives).\n",
    "And, we incorrectly predicted that {confusion_matrix.iloc[0][1]} customers would churn who then ended up not \n",
    "doing so (false positives).  There are also {confusion_matrix.iloc[1][0]} customers who ended up churning, \n",
    "that we predicted would not (false negatives).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further evaluate the model, we'll calculate the accuracy, recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from here: https://towardsdatascience.com/xgboost-in-amazon-sagemaker-28e5e354dbcd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "thresh = 0.5\n",
    "y_pred = predictions\n",
    "y_true = test_data['churn_True.']\n",
    "\n",
    "def collect_eval_metrics(true_values, predicted_values, threshold):\n",
    "    metric_df = pd.DataFrame({\"auc\":[round(roc_auc_score(true_values, predicted_values), 4)],\n",
    "                       \"accuracy\":[round(accuracy_score(true_values,(predicted_values > threshold)) ,4)],\n",
    "                       \"recall\":[round(recall_score(true_values, (predicted_values > threshold)), 4)],\n",
    "                       \"precision\":[round(precision_score(true_values, (predicted_values > threshold)),4)]})\n",
    "    return metric_df\n",
    "\n",
    "collect_eval_metrics(y_true, y_pred, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section above [Feature Importance](#Feature-Importance) we discovered that only a subset features are used in trees. If we focus on these features only will our new model perform worse or similar to the full model? We will repeat our model training exercise in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Updated model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to use the list of features from our model run above, along with our objective column (`churn_True.`) to define the list of features to retain for training. In this case, to reduce potential for errors in following this blog post, we'll freeze the list of features to match those from one of our runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_list = ['churn_True.'] + ser1[ser1 != 0].index.tolist()\n",
    "cols_list = ['churn_True.'] + ['acc_length', 'vmail_msg', 'day_mins', 'day_calls', 'eve_mins', 'night_mins', \n",
    "             'night_calls', 'int_calls', 'int_charge', 'cust_service_calls', 'int_plan_no']\n",
    "model_data_updated = model_data[cols_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_updated.head(n = 10)\n",
    "model_data_updated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the model using the same parameters but with a reduced number of features. This setup is exactly the same as for the full model above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_updated = train_data[cols_list]\n",
    "validation_data_updated = validation_data[cols_list]\n",
    "test_data_updated = test_data[cols_list]\n",
    "\n",
    "train_data_updated.to_csv('train_updated.csv', header = False, index = False)\n",
    "validation_data_updated.to_csv('validation_updated.csv', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Updated model data shape', model_data_updated.shape)\n",
    "print('Updated train data:', train_data_updated.shape)\n",
    "print('Updated validation data:', validation_data_updated.shape)\n",
    "print('Updated test data:', test_data_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train_updated/train_updated.csv')).upload_file('train_updated.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation_updated/validation_updated.csv')).upload_file('validation_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train_updated = sagemaker.s3_input(s3_data = 's3://{}/{}/train_updated/'.format(bucket, prefix),\n",
    "                               content_type='csv')\n",
    "s3_input_validation_updated = sagemaker.s3_input(s3_data = 's3://{}/{}/validation_updated/'.format(bucket, prefix),\n",
    "                                    content_type='csv')\n",
    "\n",
    "xgb_updated = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count = 1, \n",
    "                                    train_instance_type = 'ml.m4.xlarge',\n",
    "                                    output_path = 's3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session = sess)\n",
    "xgb_updated.set_hyperparameters(max_depth = 5,\n",
    "                                eta = 0.2,\n",
    "                                gamma = 4,\n",
    "                                min_child_weight = 6,\n",
    "                                subsample = 0.8,\n",
    "                                silent = 0,\n",
    "                                objective = 'binary:logistic',\n",
    "                                num_round = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the same hyperparameter_ranges and objective_metric_name as above\n",
    "tuner_updated = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs = 30,\n",
    "                            max_parallel_jobs = 3,\n",
    "                            early_stopping_type = 'Auto'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_updated.fit({'train': s3_input_train_updated, 'validation': s3_input_validation_updated}, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_updated_job_name = tuner_updated.latest_tuning_job.job_name\n",
    "tuning_updated_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_updated.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wait for approx. 30 minutes while the set of HPO jobs are run. \n",
    "\n",
    "Once it has completed, you can run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check current status of hyperparameter tuning job\n",
    "\n",
    "tuning_job_updated_result = sm_client.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName = \n",
    "                                                                            tuning_updated_job_name)\n",
    " \n",
    "status = tuning_job_updated_result['HyperParameterTuningJobStatus']\n",
    "if status != 'Completed':\n",
    "    print('Reminder: the tuning job has not been completed.')\n",
    "    \n",
    "job_count = tuning_job_updated_result['TrainingJobStatusCounters']['Completed']\n",
    "print(\"%d training jobs have completed\" % job_count)\n",
    "    \n",
    "is_minimize = (tuning_job_updated_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['Type'] != 'Maximize')\n",
    "objective_name = tuning_job_updated_result['HyperParameterTuningJobConfig']['HyperParameterTuningJobObjective']['MetricName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's review the best training job's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tuning_job_updated_result.get('BestTrainingJob',None):\n",
    "    print(\"Best model found so far:\")\n",
    "    pprint(tuning_job_updated_result['BestTrainingJob'])\n",
    "else:\n",
    "    print(\"No training jobs have reported results yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the hyperparameters used and metrics returned by the best training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_training_job_updated = tuning_job_updated_result['BestTrainingJob']['TrainingJobName']\n",
    "\n",
    "response = sm_client.describe_training_job(\n",
    "    TrainingJobName = best_training_job_updated\n",
    ")\n",
    "print('Metrics:')\n",
    "pprint(response['FinalMetricDataList'],indent=4)\n",
    "themodel = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Model location:',themodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "In this particular run, in a model with only {ser1[ser1 != 0].size} \n",
    "features our training and validation AUC values are close to but better than the run with 70 features.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host the updated model\n",
    "\n",
    "Now, we deploy the updated model to the same endpoint_name we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_updated = tuner_updated.deploy(initial_instance_count = 1,\n",
    "                                             instance_type = 'ml.m4.2xlarge',\n",
    "                                             endpoint_name = endpoint_name,\n",
    "                                             update_endpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor_updated.content_type = 'text/csv'\n",
    "xgb_predictor_updated.serializer = csv_serializer\n",
    "xgb_predictor_updated.deserializer = None\n",
    "\n",
    "predictions_updated = predict(test_data_updated.values[:, 1:], xgb_predictor_updated)\n",
    "pd.crosstab(index = test_data_updated.iloc[:, 0], \n",
    "            columns = np.round(predictions_updated), \n",
    "            rownames = ['actual'],\n",
    "            colnames = ['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our run, the new model actually performs slightly better. And, as we're using far fewer features, we'll need to pass less data for each prediction request.\n",
    "\n",
    "Let's look at some metrics for our two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "y_pred_updated = predictions_updated\n",
    "y_true_updated = test_data_updated['churn_True.']\n",
    "\n",
    "print('Updated model:')\n",
    "collect_eval_metrics(y_true_updated, y_pred_updated, thresh)\n",
    "print('Original model:')\n",
    "collect_eval_metrics(y_true, y_pred, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our runs the two models had very close results, with slight shifts; sometimes one had better precision than the other but worse recall, and with very similar accuracies.\n",
    "\n",
    "Another way to compare the two models is via a ROC curve. That will show how the AUC (area under the curve) differs between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr1, tpr1, _ = metrics.roc_curve(y_true, y_pred)\n",
    "fpr2, tpr2, _ = metrics.roc_curve(y_true_updated, y_pred_updated)\n",
    "auc_title = plt.title(\"ROC Curve\")\n",
    "auc_full_model = plt.plot(fpr1, tpr1,\n",
    "                          color = 'blue',\n",
    "                          label = \"full model\")\n",
    "auc_updated_model = plt.plot(fpr2, tpr2,\n",
    "                             color = 'green',\n",
    "                             label = \"updated model\")\n",
    "auc_legend = plt.legend(loc = 'lower right')\n",
    "random_guess = plt.plot([0,1],[0,1],'r--')\n",
    "xlim = plt.xlim([-0.1,1.1])\n",
    "ylim = plt.ylim([-0.1,1.1])\n",
    "ylabel = plt.ylabel('True Positive Rate')\n",
    "xlabel = plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f\"\"\"\n",
    "The updated model provides results very close to the original model; on some runs slightly better, on others, \n",
    "slightly worse. \n",
    "\n",
    "As the difference between the two models is below our margin of error, and the updated model requires \n",
    "far less data to be passed in the call ({ser1[ser1 != 0].size} values rather than 70), we will use the \n",
    "updated model.\n",
    "\n",
    "Lastly, here's the list of columns used for the final model. This list will be used to create the \n",
    "SQL function that calls the model for inference. Creating that will be the task of the Part 3 notebook.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_updated.columns.tolist()[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Assessing business impact\n",
    "\n",
    "We can also assess the model performance by looking at the prediction scores and refining the threshold used to decide if someone is a churner. While it’s usual to treat this as a binary classification (‘1’ or ‘0’), in fact, the real world is less binary: people become “likely to churn” for some time before they actually churn. Loss of “brand loyalty” occurs some time before someone actually buys from a competitor.\n",
    "\n",
    "Let's begin by looking at the predicted churners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data_updated.iloc[:, 0], columns=np.round(predictions_updated), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note, due to randomized elements of the algorithm, your results may differ slightly._\n",
    "\n",
    "Of the 48 churners, we've correctly predicted 34 of them (true positives). And, we incorrectly predicted 5 customers would churn who then ended up not doing so (false positives).  There are also 14 customers who ended up churning, that we predicted would not (false negatives).\n",
    "\n",
    "An important point here is that because of the `np.round()` function above we are using a simple threshold (or cutoff) of 0.5.  Our predictions from `xgboost` come out as continuous values between 0 and 1 and we force them into the binary classes that we began with.  However, because a customer that churns is expected to cost the company more than proactively trying to retain a customer who we think might churn, we should consider adjusting this cutoff.  That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_hist = plt.hist(predictions_updated, color = \"#4daf4a\")\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Number of predictions')\n",
    "plt.title('Prediction Score Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous valued predictions coming from our model tend to skew toward 0 or 1, but there is sufficient mass between 0.1 and 0.9 that adjusting the cutoff should indeed shift a number of customers' predictions.  For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.3, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that changing the cutoff from 0.5 to 0.3 results in 5 more true positives, 9 more false positives, and 5 fewer false negatives.  The numbers are small overall here, but that's 6-10% of customers overall that are shifting because of a change to the cutoff.  Was this the right decision?  We may end up retaining 9 extra customers, but we also unnecessarily incentivized 5 more customers who would have stayed.  Determining optimal cutoffs is a key step in properly applying machine learning in a real-world setting.  Let's discuss this more broadly and then apply a specific, hypothetical solution for our current problem.\n",
    "\n",
    "### Relative cost of errors\n",
    "\n",
    "Any practical binary classification problem is likely to produce a similarly sensitive cutoff. That by itself isn’t a problem. After all, if the scores for two classes are really easy to separate, the problem probably isn’t very hard to begin with and might even be solvable with simple rules instead of ML.\n",
    "\n",
    "More important, if I put an ML model into production, there are costs associated with the model erroneously assigning false positives and false negatives. I also need to look at similar costs associated with correct predictions of true positives and true negatives.  Because the choice of the cutoff affects all four of these statistics, I need to consider the relative costs to the business for each of these four outcomes for each prediction.\n",
    "\n",
    "#### Assigning costs\n",
    "\n",
    "What are the costs for our problem of mobile operator churn? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.\n",
    "\n",
    "First, we'll assign the true negatives the cost of \\\\$0. Our model essentially correctly identified a happy customer in this case, and we won’t offer them an incentive. An alternative is to assign the true negatives the actual value of the customer's spend, as this is the customer's contribution to our overall revenue. \n",
    "\n",
    "False negatives are the most problematic, because they incorrectly predict that a churning customer will stay. We lose the customer and will have to pay all the costs of acquiring a replacement customer, including foregone revenue, advertising costs, administrative costs, point of sale costs, and likely a phone hardware subsidy. Our marketing department should be able to give us a value to use here for the overhead, and we have the actual customer spend for each customer in our dataset. \n",
    "\n",
    "In the meantime, a quick search on the Internet reveals that such costs typically run in the hundreds of dollars so, for the right now, let's assume $500. This is the cost we'll use for each false negative.\n",
    "\n",
    "Finally, for customers that our model identifies as churning, we'll be giving them an incentive. At this point Marketing has not yet told us the incentives they'd like to use, so let's assume a retention incentive in the amount of \\$50. This is the cost we'll apply to both true positive and false positive outcomes. In the case of false positives (the customer is happy, but the model mistakenly predicted churn), we will “waste” the concession. We probably could have spent those dollars more effectively, but it's possible we increased the loyalty of an already loyal customer, so that’s not so bad.\n",
    "\n",
    "Once we have the new incentive programs from Marketing, we can rerun this analysis, using the actual customer spend and the planned incentives to calculate the cost of each prediction. That will allow us to provide an economic model of the effect of the proposed incentive program when it's put into production along with the predictions from this model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal threshold\n",
    "\n",
    "It’s clear that false negatives are substantially more costly than false positives. Instead of optimizing for error based on the number of customers, we should be minimizing a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "cost_of_replacing_customer * FN(C) + customer_value * TN(C) + incentive_offered * FP(C) + incentive_offered * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "For even better outcomes, we could even offer a range of incentives to customers that meet different criteria. For example, it's worth more to the business to prevent a high spend customer from churning than a low spend customer. We could also target the \"grey area\" of customers that have less loyalty and could be swayed by another company's advertising. [This blog](https://aws.amazon.com/blogs/machine-learning/training-models-with-unequal-economic-error-costs-using-amazon-sagemaker/) and [this one](https://aws.amazon.com/blogs/machine-learning/optimizing-portfolio-value-with-amazon-sagemaker-automatic-model-tuning/) provide some examples and ideas on how this can be accomplished. \n",
    "\n",
    "Right now we'll start by using the same values for all customers, to give us a starting point for discussion with the business. With the estimates we'll use for right now, this becomes:\n",
    "\n",
    "```txt\n",
    "$500 * FN(C) + $0 * TN(C) + $50 * FP(C) + $50 * TP(C)\n",
    "```\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "cutoffs = np.arange(predictions_updated.min(), predictions_updated.max(), 0.01)\n",
    "costs = []\n",
    "fn = 500\n",
    "tn = 0\n",
    "fp = 50\n",
    "tp = 50\n",
    "for c in cutoffs:\n",
    "    costs.append(np.sum(np.sum(np.array([[tn, tp], [fn, fp]]) * \n",
    "                               pd.crosstab(index=test_data_updated.iloc[:, 0], \n",
    "                                           columns=np.where(predictions_updated > c, 1, 0)))))\n",
    "\n",
    "costs = np.array(costs)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(cutoffs, costs)\n",
    "fmt = '${x:,.0f}'\n",
    "tick = ticker.StrMethodFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(tick) \n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Threshold versus Cost')\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)], 'for a cost of: $', np.min(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows how picking a threshold too low results in costs skyrocketing as all customers are given a retention incentive.  Meanwhile, setting the threshold too high (e.g., 0.7 or above) results in too many lost customers, which ultimately grows to be nearly as costly. In between, there is a large \"grey\" area, where perhaps some more nuanced incentives would create better outcomes.  \n",
    "\n",
    "The overall cost can be minimized at \\\\$5950 by setting the cutoff to 0.12, which is substantially better than the \\\\$20k+ I would expect to lose by not taking any action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Now, we have a working model, with an endpoint we can call that will tell us whether a specific customer is predicted to churn.\n",
    "\n",
    "There are a number of techniques that can be used to provide deeper explanations of this model, and of specific predictions; for example, popular techniques include [SHAP](https://github.com/slundberg/shap) and [LIME](https://github.com/marcotcr/lime). \n",
    "\n",
    "There are also a number of opportunities to further improve this model. For example, by evaluating the model based on the economic cost of losing versus keeping a customer, we can shift the model's error rates between false positives and false negatives. We can also optimize how much we're willing to spend in order to keep a customer. Look at the following blogs for ideas:\n",
    "\n",
    "* [Training models with unequal economic error costs using Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/training-models-with-unequal-economic-error-costs-using-amazon-sagemaker/)\n",
    "\n",
    "* [Optimizing portfolio value with Amazon SageMaker automatic model tuning](https://aws.amazon.com/blogs/machine-learning/optimizing-portfolio-value-with-amazon-sagemaker-automatic-model-tuning/)\n",
    "\n",
    "Other means of extending it include:\n",
    "- Some customers who receive retention incentives will still churn.  Including a probability of churning despite receiving an incentive in our cost function would provide a better ROI on our retention programs.\n",
    "- Customers who switch to a lower-priced plan or who deactivate a paid feature represent different kinds of churn that could be modeled separately.\n",
    "- Modeling the evolution of customer behavior. If usage is dropping and the number of calls placed to Customer Service is increasing, you are more likely to experience churn then if the trend is the opposite. A customer profile should incorporate behavior trends.\n",
    "- Actual training data and monetary cost assignments could be more complex.\n",
    "- Multiple models for each type of churn could be needed.\n",
    "\n",
    "Regardless of additional complexity, similar principles described in this notebook are likely apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Next Steps**\n",
    "\n",
    "Now, move on to the next (and last) notebook - Part 3. In Part 3, you'll connect the updated endpoint you just created to Aurora, and call it from your 'production workflow'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Clean-up (AFTER you've run the Part 3 notebook)\n",
    "\n",
    "If you're ready to be done with this use case, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on.\n",
    "\n",
    "However if you remove it before you've completed Part 3, you will not be able to make inferences against the endpoint!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "sagemaker.Session().delete_endpoint(xgb_predictor_updated.endpoint)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
